# Lecture Notes for "The Neuroscience of Decision Making" IrBO26 Summer Camp

- Slide 5
  - When a large loss is associated with a false alarm, then a more conservative strategy is called for, and vice versa. The overall loss size for false alarms is determined by the *loss value associated a single false alarm* times the *probability of a false alarm*. Therefore the appropriate criterion depends on the *relative cost of the errors* and the *prior probability* of encountering the signal.
- Slide 9
  - This equation is also termed Bayes's Theorem despite the fact that it follows from simple laws of probability.
- Slide 10
  - As can be seen from the equation, *d'* can increase through either increasing the separation of the means or by reducing the variance of the distributions.
- Slide 11
  - Make sure to note that the power and size are both functions of some threshold.
- Slide 13
  - Here, the minimum threshold value is assumed to be 0 since we are discussing signal detection in the context of neural firing rates, and negative firing rates are physically impossible.
- Slide 16
  - The term $r_{\mathrm{ave}}$ in the second equation is the average of the two Gaussians' means ($\frac{r_++r_-}{2}$) and $\sigma_r$ is the standard deviation of the Gaussians.
- Slide 18
  - Sections 3.1 and 3.2
- Slide 22
  - Read the figure from left to right
- Slide 24
  - Task difficulty can also be increased by making the directions of motion more similar, but decreasing the coherence is a more convenient option.
- Slide 25
  - You can find this video [here](https://www.youtube.com/watch?v=oDxcyTn-0os).
- Slide 26
  - Make sure to notice that the x-axis is not linear and that $\mathbb{P}(\mathrm{correct})=\frac{1}{2}$ at 0% coherence.
- Slide 27
  - Area MT is also called V5 or visual area 5.
- Slide 30
  - We can't initially posit direct inhibitory connections between our two excitatory populations due to [Dale's Law](https://en.wikipedia.org/wiki/Dale%27s_principle): neurons, and therefore populations of homogeneous neurons, cannot project both excitatory and inhibitory connections.
- Slide 31
  - We call an input unbiased if $\mathrm{mean}(I_1)=\mathrm{mean}(I_2)$.
- Slide 32
  - The gain function $g_\sigma(I)$ relates the current at neuronal population $\sigma$ to its firing rate $A$.
  - The equation describing $h$ is Ohm's law convolved by $e^{-t}$. Think of it as resistance $R$ times the sum of all currents ever injected into this population weighted by $e^{t-s}$, where $s$ is how far away this time point is in the past.
- Slide 33
  - Assumption 1 means that the dynamics of the inhibitory population is instantaneous and its potential is always at the fixed point $h_\mathrm{I}=w_\mathrm{IE}\big[g_\mathrm{E}(h_{\mathrm{E,1}})+g_\mathrm{E}(h_{\mathrm{E,2}})\big]$.
  - Since the gain function has a positive slope (higher input currents lead to higher firing rates), $\gamma$ is a positive constant.
- Slide 34
  - These dynamics also hold for small unbiased inputs.
- Slide 35
  - A saddle point is an *unstable* equilibrium point.
- Slide 39
  - The stimulated neurons simply add a small amount of evidence for rightward motion, so they only change the total signal that the brain uses to make its decision by a small amount. This is why microstimulation has a larger observable effect in near-zero motion strengths.
- Slide 41
  - Note that decisions that take longer are more accurate *given some constant motion strength*. If we allow the motion strength to vary, longer decision times will be associated with weaker motion strengths and will therefore be less accurate.
- Slide 42
  - The dotted circles are response fields.
- Slide 42
  - This figure charts firing rate vs. time only for neurons that have a response field to the right (dotted circles) and only when the monkey made a correct choice.
- Slide 50
  - The normalization factor $\frac{1}{\mathbb{P}(x_1,x_2)}$ has been left out since we can normalize at the final stage.
  - The prior $\mathbb{P}(s)$ has been left out in the graph since it's a uniform distribution.
- Slide 51
  - Refer to Ma (2021) problem 3.3 for a guide to deriving the product of two Gaussians.
- Slide 54
  - Remember that the variance of the posterior is guaranteed to shrink *only if* the measurements are conditionally independent.
- Slide 57
  - In this setup a $\mathrm{LR}>1$ or $\log\mathrm{LR}>0$ is evidence in favor of green.
